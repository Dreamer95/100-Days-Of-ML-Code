import os
import sys
import argparse
import glob
import joblib
import numpy as np
from datetime import datetime
import pytz

# Ensure relative imports work when running from project root or Src/Day5
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)

from advanced_traffic_predictor import AdvancedTrafficPredictor
from newrelic_traffic_collector import get_recent_3h_1min_dataframe


def predict_from_inputs(
    when,
    current_tpm: float,
    previous_tpms: list = None,
    response_time: float = None,
    push_notification_active: int = 0,
    minutes_since_push: int = 999999,
    minutes_ahead=(5, 10, 15),
    extra_features: dict = None,
    models_dir: str = None,
    timestamp: str = None,
):
    """
    Dá»± Ä‘oÃ¡n TPM cho cÃ¡c horizon (5/10/15 phÃºt) tá»« input tuá»³ chá»‰nh, sá»­ dá»¥ng
    cÃ¡c model Ä‘Ã£ Ä‘Æ°á»£c train vÃ  lÆ°u trÆ°á»›c Ä‘Ã³ (khÃ´ng cáº§n train láº¡i).

    Tham sá»‘:
    - when: datetime (naive hoáº·c timezone-aware)
    - current_tpm: TPM táº¡i thá»i Ä‘iá»ƒm 'when'
    - previous_tpms: list TPM quÃ¡ khá»©, pháº§n tá»­ 0 lÃ  1 phÃºt trÆ°á»›c, [1] lÃ  2 phÃºt trÆ°á»›c, ...
    - response_time: response time hiá»‡n táº¡i (náº¿u cÃ³)
    - push_notification_active: cá» 0/1 cho biáº¿t cÃ³ push táº¡i thá»i Ä‘iá»ƒm 'when'
    - minutes_since_push: sá»‘ phÃºt ká»ƒ tá»« láº§n push gáº§n nháº¥t
    - minutes_ahead: tuple/list cÃ¡c horizon cáº§n dá»± Ä‘oÃ¡n (máº·c Ä‘á»‹nh 5/10/15)
    - extra_features: dict cÃ¡c feature bá»• sung náº¿u model cÃ³
    - models_dir: thÆ° má»¥c chá»©a models (máº·c Ä‘á»‹nh Src/Day5/models)
    - timestamp: timestamp model cá»¥ thá»ƒ Ä‘á»ƒ load; náº¿u None sáº½ tá»± láº¥y báº£n má»›i nháº¥t

    Tráº£ vá»:
    - dict: { 'tpm_5min': float, 'tpm_10min': float, 'tpm_15min': float }
    """
    # XÃ¡c Ä‘á»‹nh thÆ° má»¥c models máº·c Ä‘á»‹nh theo cáº¥u trÃºc project
    if models_dir is None:
        models_dir = os.path.join(CURRENT_DIR, 'models')

    if not os.path.isdir(models_dir):
        raise FileNotFoundError(f"Models directory not found: {models_dir}. HÃ£y cháº¡y training Ä‘á»ƒ táº¡o models trÆ°á»›c.")

    # Khá»Ÿi táº¡o predictor vÃ  náº¡p models theo timestamp má»›i nháº¥t (hoáº·c theo tham sá»‘)
    predictor = AdvancedTrafficPredictor()

    # TÃ¬m timestamp má»›i nháº¥t náº¿u chÆ°a cung cáº¥p
    if not timestamp:
        ts = find_latest_timestamp(models_dir)
        if not ts:
            raise FileNotFoundError("No saved models found. Please run training to create models.")
        timestamp = ts

    ok = load_models_into_predictor(predictor, models_dir, timestamp)
    if not ok:
        raise RuntimeError("Failed to load models/scalers/metadata. Cannot perform prediction.")

    # Gá»i vÃ o hÃ m suy luáº­n Ä‘iá»ƒm-thá»i-gian cá»§a predictor
    return predictor.predict_from_inputs(
        when=when,
        current_tpm=current_tpm,
        previous_tpms=previous_tpms,
        response_time=response_time,
        push_notification_active=push_notification_active,
        minutes_since_push=minutes_since_push,
        minutes_ahead=minutes_ahead,
        extra_features=extra_features,
    )


def find_latest_timestamp(models_dir: str) -> str:
    """Find the latest metadata_<timestamp>.joblib and return its timestamp string."""
    pattern = os.path.join(models_dir, 'metadata_*.joblib')
    files = glob.glob(pattern)
    if not files:
        return None
    # Sort by timestamp parsed from filename or by mtime as fallback
    def extract_ts(path):
        base = os.path.basename(path)
        ts = base.replace('metadata_', '').replace('.joblib', '')
        return ts
    files_sorted = sorted(files, key=lambda p: extract_ts(p))
    return extract_ts(files_sorted[-1])


def load_models_into_predictor(predictor: AdvancedTrafficPredictor, models_dir: str, timestamp: str) -> bool:
    """Load best models, scalers, and metadata into the predictor for a given timestamp.
    Returns True if successful, False otherwise.
    """
    metadata_path = os.path.join(models_dir, f'metadata_{timestamp}.joblib')
    if not os.path.exists(metadata_path):
        print(f"âŒ Metadata not found: {metadata_path}")
        return False

    metadata = joblib.load(metadata_path)
    best_models = metadata.get('best_models', {})
    feature_columns = metadata.get('feature_columns')
    target_columns = metadata.get('target_columns')

    if not best_models:
        print("âŒ No best_models found in metadata; cannot load models.")
        return False

    # Initialize containers matching training structure
    predictor.models = {t: {} for t in target_columns}
    predictor.scalers = {}
    predictor.best_models = best_models
    predictor.model_performance = metadata.get('model_performance', {})

    # Load thresholds from metadata if available
    predictor.tpm_thresholds = metadata.get('tpm_thresholds') or metadata.get('thresholds')
    predictor.tpm_thresholds_stats = metadata.get('tpm_thresholds_stats')

    # If feature/target columns differ, update to match saved artifacts
    if feature_columns:
        predictor.feature_columns = feature_columns
    if target_columns:
        predictor.target_columns = target_columns

    # ThÃ´ng tin tá»•ng quan trÆ°á»›c khi load tá»«ng model
    print("\nğŸ§  Model Summary")
    print(f"   â€¢ Timestamp: {timestamp}")
    print(f"   â€¢ Models dir: {models_dir}")
    print(f"   â€¢ #Features: {len(predictor.feature_columns) if predictor.feature_columns else 0}")
    if predictor.feature_columns:
        print(f"   â€¢ Feature columns: {predictor.feature_columns}")
    print(f"   â€¢ #Targets: {len(predictor.target_columns) if predictor.target_columns else 0}")
    if predictor.target_columns:
        print(f"   â€¢ Target columns: {predictor.target_columns}")

    # Load best model per target and its scaler
    for target, best_name in best_models.items():
        model_path = os.path.join(models_dir, f'{target}_{best_name}_{timestamp}.joblib')
        if not os.path.exists(model_path):
            print(f"âŒ Missing model file: {model_path}")
            return False
        model_obj = joblib.load(model_path)
        predictor.models[target][best_name] = model_obj

        # Load scaler for the target (always saved, used particularly for SVR)
        scaler_path = os.path.join(models_dir, f'scaler_{target}_{timestamp}.joblib')
        scaler_loaded = False
        if not os.path.exists(scaler_path):
            print(f"âš ï¸ Scaler not found for {target}: {scaler_path}. Proceeding without scaler.")
        else:
            predictor.scalers[target] = joblib.load(scaler_path)
            scaler_loaded = True

        # In thÃªm thÃ´ng tin chi tiáº¿t cho tá»«ng target/model
        model_class = type(model_obj).__name__
        print(f"\n   â¤ Target: {target}")
        print(f"     - Best model key: {best_name}")
        print(f"     - Estimator class: {model_class}")
        print(f"     - Model file: {os.path.basename(model_path)}")
        print(f"     - Scaler: {'loaded' if scaler_loaded else 'not found'}")

        # ThÃ´ng tin hiá»‡u nÄƒng (náº¿u cÃ³ trong metadata.model_performance)
        perf_info = predictor.model_performance.get(target, {})
        best_perf = perf_info.get(best_name)
        if isinstance(best_perf, dict) and best_perf:
            metrics_str = ", ".join(f"{k}: {v:.4f}" if isinstance(v, (int, float)) else f"{k}: {v}"
                                    for k, v in best_perf.items())
            print(f"     - Performance: {metrics_str}")
        elif perf_info:
            print(f"     - Performance (raw): {perf_info}")

    # Náº¿u thresholds chÆ°a cÃ³, thá»­ tÃ­nh tá»« training CSV
    if predictor.tpm_thresholds is None:
        try:
            if predictor.data_file_path and os.path.exists(predictor.data_file_path):
                df_train = predictor.load_real_data()
                predictor.tpm_thresholds = predictor.compute_tpm_thresholds_from_df(df_train)
                print(f"ğŸ§® Computed thresholds from training CSV: {predictor.tpm_thresholds}")
            else:
                print("âš ï¸ No thresholds in metadata and training CSV not found; labels may be 'unknown'.")
        except Exception as e:
            print(f"âš ï¸ Failed to compute thresholds from training CSV: {e}")

    # TÃ³m táº¯t thresholds
    if predictor.tpm_thresholds:
        th = predictor.tpm_thresholds
        try:
            print(f"   â€¢ Thresholds: q60={th['q60']:.2f}, q80={th['q80']:.2f}, q90={th['q90']:.2f}")
        except Exception:
            print(f"   â€¢ Thresholds: {th}")

    print(f"\nâœ… Loaded models and metadata from timestamp {timestamp}")
    return True


def main():
    parser = argparse.ArgumentParser(description='Run demo predictions without retraining (using live New Relic data)')
    parser.add_argument('--hours-ahead', type=int, default=6, help='Hours ahead to scan for high TPM periods')
    parser.add_argument('--dataset', type=str, default=None, help='Path to dataset CSV as a fallback if API fails')
    parser.add_argument('--timestamp', type=str, default=None, help='Specific model timestamp to load (e.g., 20250809_224101)')
    parser.add_argument('--api-key', type=str, default=os.environ.get('NEWRELIC_API_KEY') or os.environ.get('NEW_RELIC_API_KEY'), help='New Relic API key (default from env NEWRELIC_API_KEY)')
    parser.add_argument('--app-id', type=str, default=os.environ.get('NEWRELIC_APP_ID') or os.environ.get('NEW_RELIC_APP_ID'), help='New Relic Application ID (default from env NEWRELIC_APP_ID)')
    parser.add_argument('--metric', type=str, default='HttpDispatcher', help='Metric name to query (default: HttpDispatcher)')
    args = parser.parse_args()

    models_dir = os.path.join(CURRENT_DIR, 'models')
    if not os.path.isdir(models_dir):
        print(f"âŒ Models directory not found: {models_dir}\nPlease run the training pipeline first to generate models.")
        sys.exit(1)

    # Determine timestamp to load
    timestamp = args.timestamp or find_latest_timestamp(models_dir)
    if not timestamp:
        print("âŒ No saved models found. Please run training to create models.")
        sys.exit(1)

    # Initialize predictor
    predictor = AdvancedTrafficPredictor()

    # Prefer live New Relic data (last 3 hours, 1-min granularity)
    print("ğŸŒ Fetching recent 3h data (1-min) from New Relic API...")
    live_df = get_recent_3h_1min_dataframe(api_key=args.api_key, app_id=args.app_id, metric_name=args.metric)

    if live_df is None or live_df.empty:
        if args.dataset:
            print("âš ï¸ API data unavailable. Falling back to dataset CSV provided via --dataset.")
            predictor.data_file_path = args.dataset
            df = predictor.load_and_preprocess_data()
        else:
            print("âŒ No live data fetched and no dataset fallback provided. Aborting.")
            sys.exit(1)
    else:
        # Giáº£ láº­p push vá»«a diá»…n ra táº¡i thá»i Ä‘iá»ƒm cuá»‘i cÃ¹ng trong live_df
        # push_time = live_df['timestamp'].max()
        # window_mins = 15  # Ä‘á»™ dÃ i khoáº£ng thá»i gian "active"
        #
        # # TÃ­nh phÃºt ká»ƒ tá»« push (Ã¢m trÆ°á»›c push, >=0 sau push)
        # delta_min = (live_df['timestamp'] - push_time).dt.total_seconds() / 60.0
        #
        # live_df['minutes_since_push'] = delta_min.where(delta_min >= 0, other=np.nan)
        # # Active trong 15 phÃºt sau push
        # live_df['push_notification_active'] = ((delta_min >= 0) & (delta_min <= window_mins)).astype(int)
        #
        # # Äiá»n giÃ¡ trá»‹ máº·c Ä‘á»‹nh cho cÃ¡c dÃ²ng trÆ°á»›c push
        # live_df['minutes_since_push'] = live_df['minutes_since_push'].fillna(999999)

        # Create features from live data for prediction (keep latest rows even if some targets are NaN)
        df = predictor.create_features(live_df)

    # Load models into predictor
    ok = load_models_into_predictor(predictor, models_dir, timestamp)
    if not ok:
        print("âŒ Failed to load models. Aborting.")
        sys.exit(1)

    # Run the demo predictions without retraining
    print("\nğŸš€ Running independent demo predictions...")
    predictor.demonstrate_predictions(df, hours_ahead=args.hours_ahead)


def main2():
    # Láº¥y ngÃ y hÃ´m nay 09:00 táº¡i GMT+7
    tz = pytz.timezone('Asia/Bangkok')
    date_str = '2025-08-10 15:30'
    naive_dt = datetime.strptime(date_str, '%Y-%m-%d %H:%M')
    test_when = tz.localize(naive_dt)

    # GiÃ¡ trá»‹ máº«u Ä‘á»ƒ test
    current_tpm = 125.0
    previous_tpms = [120.0, 100.0, 123.0, 124.0, 135.0,120.0, 100.0, 123.0, 124.0, 135.0,120.0, 100.0, 123.0, 124.0, 135.0]  # 1â†’5 phÃºt trÆ°á»›c
    response_time = 120.0

    # MÃ´ phá»ng vá»«a push lÃºc 09:00
    push_notification_active = 0
    minutes_since_push = 3000

    print("\nğŸ§ª Testing predict_from_inputs at 09:00 GMT+7 ...")
    result = predict_from_inputs(
        when=test_when,
        current_tpm=current_tpm,
        previous_tpms=previous_tpms,
        response_time=response_time,
        push_notification_active=push_notification_active,
        minutes_since_push=minutes_since_push,
        minutes_ahead=(5, 10, 15),
        # models_dir=None, timestamp=None  # dÃ¹ng máº·c Ä‘á»‹nh: láº¥y model má»›i nháº¥t tá»« thÆ° má»¥c models
    )
    preds = result.get('predictions', {})
    labels = result.get('labels', {})

    print(f"ğŸ§ª Predictions at {test_when.strftime('%Y-%m-%d %H:%M %Z')}:")
    for key in sorted(preds.keys(), key=lambda k: int(k.split('_')[1].replace('min', ''))):
        minutes = key.split('_')[1].replace('min', '')
        value = preds[key]
        label = labels.get(key, 'unknown')
        print(f"  â€¢ t+{minutes}m: {value:.2f} ({label})")


if __name__ == '__main__':
    # main2()
    main()

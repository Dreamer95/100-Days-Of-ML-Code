import os
import sys
import argparse
import glob
import joblib
from datetime import datetime
import pytz

# Ensure relative imports work when running from project root or Src/Day5
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)

from advanced_traffic_predictor import AdvancedTrafficPredictor
from newrelic_traffic_collector import get_recent_3h_1min_dataframe


def predict_from_inputs(
    when,
    current_tpm: float,
    previous_tpms: list = None,
    response_time: float = None,
    push_notification_active: int = 0,
    minutes_since_push: int = 999999,
    minutes_ahead=(5, 10, 15),
    extra_features: dict = None,
    models_dir: str = None,
    timestamp: str = None,
):
    """
    D·ª± ƒëo√°n TPM cho c√°c horizon (5/10/15 ph√∫t) t·ª´ input tu·ª≥ ch·ªânh, s·ª≠ d·ª•ng
    c√°c model ƒë√£ ƒë∆∞·ª£c train v√† l∆∞u tr∆∞·ªõc ƒë√≥ (kh√¥ng c·∫ßn train l·∫°i).

    Tham s·ªë:
    - when: datetime (naive ho·∫∑c timezone-aware)
    - current_tpm: TPM t·∫°i th·ªùi ƒëi·ªÉm 'when'
    - previous_tpms: list TPM qu√° kh·ª©, ph·∫ßn t·ª≠ 0 l√† 1 ph√∫t tr∆∞·ªõc, [1] l√† 2 ph√∫t tr∆∞·ªõc, ...
    - response_time: response time hi·ªán t·∫°i (n·∫øu c√≥)
    - push_notification_active: c·ªù 0/1 cho bi·∫øt c√≥ push t·∫°i th·ªùi ƒëi·ªÉm 'when'
    - minutes_since_push: s·ªë ph√∫t k·ªÉ t·ª´ l·∫ßn push g·∫ßn nh·∫•t
    - minutes_ahead: tuple/list c√°c horizon c·∫ßn d·ª± ƒëo√°n (m·∫∑c ƒë·ªãnh 5/10/15)
    - extra_features: dict c√°c feature b·ªï sung n·∫øu model c√≥
    - models_dir: th∆∞ m·ª•c ch·ª©a models (m·∫∑c ƒë·ªãnh Src/Day5/models)
    - timestamp: timestamp model c·ª• th·ªÉ ƒë·ªÉ load; n·∫øu None s·∫Ω t·ª± l·∫•y b·∫£n m·ªõi nh·∫•t

    Tr·∫£ v·ªÅ:
    - dict: { 'tpm_5min': float, 'tpm_10min': float, 'tpm_15min': float }
    """
    # X√°c ƒë·ªãnh th∆∞ m·ª•c models m·∫∑c ƒë·ªãnh theo c·∫•u tr√∫c project
    if models_dir is None:
        models_dir = os.path.join(CURRENT_DIR, 'models')

    if not os.path.isdir(models_dir):
        raise FileNotFoundError(f"Models directory not found: {models_dir}. H√£y ch·∫°y training ƒë·ªÉ t·∫°o models tr∆∞·ªõc.")

    # Kh·ªüi t·∫°o predictor v√† n·∫°p models theo timestamp m·ªõi nh·∫•t (ho·∫∑c theo tham s·ªë)
    predictor = AdvancedTrafficPredictor()

    # T√¨m timestamp m·ªõi nh·∫•t n·∫øu ch∆∞a cung c·∫•p
    if not timestamp:
        ts = find_latest_timestamp(models_dir)
        if not ts:
            raise FileNotFoundError("No saved models found. Please run training to create models.")
        timestamp = ts

    ok = load_models_into_predictor(predictor, models_dir, timestamp)
    if not ok:
        raise RuntimeError("Failed to load models/scalers/metadata. Cannot perform prediction.")

    # G·ªçi v√†o h√†m suy lu·∫≠n ƒëi·ªÉm-th·ªùi-gian c·ªßa predictor
    return predictor.predict_from_inputs(
        when=when,
        current_tpm=current_tpm,
        previous_tpms=previous_tpms,
        response_time=response_time,
        push_notification_active=push_notification_active,
        minutes_since_push=minutes_since_push,
        minutes_ahead=minutes_ahead,
        extra_features=extra_features,
    )


def find_latest_timestamp(models_dir: str) -> str:
    """Find the latest metadata_<timestamp>.joblib and return its timestamp string."""
    pattern = os.path.join(models_dir, 'metadata_*.joblib')
    files = glob.glob(pattern)
    if not files:
        return None
    # Sort by timestamp parsed from filename or by mtime as fallback
    def extract_ts(path):
        base = os.path.basename(path)
        ts = base.replace('metadata_', '').replace('.joblib', '')
        return ts
    files_sorted = sorted(files, key=lambda p: extract_ts(p))
    return extract_ts(files_sorted[-1])


def load_models_into_predictor(predictor: AdvancedTrafficPredictor, models_dir: str, timestamp: str) -> bool:
    """Load best models, scalers, and metadata into the predictor for a given timestamp.
    Returns True if successful, False otherwise.
    """
    metadata_path = os.path.join(models_dir, f'metadata_{timestamp}.joblib')
    if not os.path.exists(metadata_path):
        print(f"‚ùå Metadata not found: {metadata_path}")
        return False

    metadata = joblib.load(metadata_path)
    best_models = metadata.get('best_models', {})
    feature_columns = metadata.get('feature_columns')
    target_columns = metadata.get('target_columns')

    if not best_models:
        print("‚ùå No best_models found in metadata; cannot load models.")
        return False

    # Initialize containers matching training structure
    predictor.models = {t: {} for t in target_columns}
    predictor.scalers = {}
    predictor.best_models = best_models
    predictor.model_performance = metadata.get('model_performance', {})

    # If feature/target columns differ, update to match saved artifacts
    if feature_columns:
        predictor.feature_columns = feature_columns
    if target_columns:
        predictor.target_columns = target_columns

    # Load best model per target and its scaler
    for target, best_name in best_models.items():
        model_path = os.path.join(models_dir, f'{target}_{best_name}_{timestamp}.joblib')
        if not os.path.exists(model_path):
            print(f"‚ùå Missing model file: {model_path}")
            return False
        predictor.models[target][best_name] = joblib.load(model_path)

        # Load scaler for the target (always saved, used particularly for SVR)
        scaler_path = os.path.join(models_dir, f'scaler_{target}_{timestamp}.joblib')
        if not os.path.exists(scaler_path):
            print(f"‚ö†Ô∏è Scaler not found for {target}: {scaler_path}. Proceeding without scaler.")
        else:
            predictor.scalers[target] = joblib.load(scaler_path)

    print(f"‚úÖ Loaded models and metadata from timestamp {timestamp}")
    return True


def main():
    parser = argparse.ArgumentParser(description='Run demo predictions without retraining (using live New Relic data)')
    parser.add_argument('--hours-ahead', type=int, default=6, help='Hours ahead to scan for high TPM periods')
    parser.add_argument('--dataset', type=str, default=None, help='Path to dataset CSV as a fallback if API fails')
    parser.add_argument('--timestamp', type=str, default=None, help='Specific model timestamp to load (e.g., 20250809_224101)')
    parser.add_argument('--api-key', type=str, default=os.environ.get('NEWRELIC_API_KEY') or os.environ.get('NEW_RELIC_API_KEY'), help='New Relic API key (default from env NEWRELIC_API_KEY)')
    parser.add_argument('--app-id', type=str, default=os.environ.get('NEWRELIC_APP_ID') or os.environ.get('NEW_RELIC_APP_ID'), help='New Relic Application ID (default from env NEWRELIC_APP_ID)')
    parser.add_argument('--metric', type=str, default='HttpDispatcher', help='Metric name to query (default: HttpDispatcher)')
    args = parser.parse_args()

    models_dir = os.path.join(CURRENT_DIR, 'models')
    if not os.path.isdir(models_dir):
        print(f"‚ùå Models directory not found: {models_dir}\nPlease run the training pipeline first to generate models.")
        sys.exit(1)

    # Determine timestamp to load
    timestamp = args.timestamp or find_latest_timestamp(models_dir)
    if not timestamp:
        print("‚ùå No saved models found. Please run training to create models.")
        sys.exit(1)

    # Initialize predictor
    predictor = AdvancedTrafficPredictor()

    # Prefer live New Relic data (last 3 hours, 1-min granularity)
    print("üåê Fetching recent 3h data (1-min) from New Relic API...")
    live_df = get_recent_3h_1min_dataframe(api_key=args.api_key, app_id=args.app_id, metric_name=args.metric)

    if live_df is None or live_df.empty:
        if args.dataset:
            print("‚ö†Ô∏è API data unavailable. Falling back to dataset CSV provided via --dataset.")
            predictor.data_file_path = args.dataset
            df = predictor.load_and_preprocess_data()
        else:
            print("‚ùå No live data fetched and no dataset fallback provided. Aborting.")
            sys.exit(1)
    else:
        # Create features from live data for prediction (keep latest rows even if some targets are NaN)
        df = predictor.create_features(live_df)

    # Load models into predictor
    ok = load_models_into_predictor(predictor, models_dir, timestamp)
    if not ok:
        print("‚ùå Failed to load models. Aborting.")
        sys.exit(1)

    # Run the demo predictions without retraining
    print("\nüöÄ Running independent demo predictions...")
    predictor.demonstrate_predictions(df, hours_ahead=args.hours_ahead)


def main2():
    # L·∫•y ng√†y h√¥m nay 09:00 t·∫°i GMT+7
    tz = pytz.timezone('Asia/Bangkok')
    date_str = '2025-08-15 09:00'
    naive_dt = datetime.strptime(date_str, '%Y-%m-%d %H:%M')
    test_when = tz.localize(naive_dt)

    # Gi√° tr·ªã m·∫´u ƒë·ªÉ test
    current_tpm = 200.0
    previous_tpms = [190.0, 100.0, 123.0, 124.0, 130.0]  # 1‚Üí5 ph√∫t tr∆∞·ªõc
    response_time = 120.0

    # M√¥ ph·ªèng v·ª´a push l√∫c 09:00
    push_notification_active = 1
    minutes_since_push = 0

    print("\nüß™ Testing predict_from_inputs at 09:00 GMT+7 ...")
    preds = predict_from_inputs(
        when=test_when,
        current_tpm=current_tpm,
        previous_tpms=previous_tpms,
        response_time=response_time,
        push_notification_active=push_notification_active,
        minutes_since_push=minutes_since_push,
        minutes_ahead=(5, 10, 15),
        # models_dir=None, timestamp=None  # d√πng m·∫∑c ƒë·ªãnh: l·∫•y model m·ªõi nh·∫•t t·ª´ th∆∞ m·ª•c models
    )
    print(f"üß™ Predictions at {test_when.strftime('%Y-%m-%d %H:%M %Z')}: {preds}")

if __name__ == '__main__':
    main2()

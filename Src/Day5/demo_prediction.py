import os
import sys
import argparse
import glob
import joblib
import numpy as np
from datetime import datetime
import pytz

# Ensure relative imports work when running from project root or Src/Day5
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)

from advanced_traffic_predictor import AdvancedTrafficPredictor
from newrelic_traffic_collector import get_recent_3h_1min_dataframe


def predict_from_inputs(
    when,
    current_tpm: float,
    previous_tpms: list = None,
    response_time: float = None,
    push_notification_active: int = 0,
    minutes_since_push: int = 999999,
    minutes_ahead=(5, 10, 15),
    extra_features: dict = None,
    models_dir: str = None,
    timestamp: str = None,
):
    """
    D·ª± ƒëo√°n TPM cho c√°c horizon (5/10/15 ph√∫t) t·ª´ input tu·ª≥ ch·ªânh, s·ª≠ d·ª•ng
    c√°c model ƒë√£ ƒë∆∞·ª£c train v√† l∆∞u tr∆∞·ªõc ƒë√≥ (kh√¥ng c·∫ßn train l·∫°i).

    Tham s·ªë:
    - when: datetime (naive ho·∫∑c timezone-aware)
    - current_tpm: TPM t·∫°i th·ªùi ƒëi·ªÉm 'when'
    - previous_tpms: list TPM qu√° kh·ª©, ph·∫ßn t·ª≠ 0 l√† 1 ph√∫t tr∆∞·ªõc, [1] l√† 2 ph√∫t tr∆∞·ªõc, ...
    - response_time: response time hi·ªán t·∫°i (n·∫øu c√≥)
    - push_notification_active: c·ªù 0/1 cho bi·∫øt c√≥ push t·∫°i th·ªùi ƒëi·ªÉm 'when'
    - minutes_since_push: s·ªë ph√∫t k·ªÉ t·ª´ l·∫ßn push g·∫ßn nh·∫•t
    - minutes_ahead: tuple/list c√°c horizon c·∫ßn d·ª± ƒëo√°n (m·∫∑c ƒë·ªãnh 5/10/15)
    - extra_features: dict c√°c feature b·ªï sung n·∫øu model c√≥
    - models_dir: th∆∞ m·ª•c ch·ª©a models (m·∫∑c ƒë·ªãnh Src/Day5/models)
    - timestamp: timestamp model c·ª• th·ªÉ ƒë·ªÉ load; n·∫øu None s·∫Ω t·ª± l·∫•y b·∫£n m·ªõi nh·∫•t

    Tr·∫£ v·ªÅ:
    - dict: { 'tpm_5min': float, 'tpm_10min': float, 'tpm_15min': float }
    """
    # X√°c ƒë·ªãnh th∆∞ m·ª•c models m·∫∑c ƒë·ªãnh theo c·∫•u tr√∫c project
    if models_dir is None:
        models_dir = os.path.join(CURRENT_DIR, 'models')

    if not os.path.isdir(models_dir):
        raise FileNotFoundError(f"Models directory not found: {models_dir}. H√£y ch·∫°y training ƒë·ªÉ t·∫°o models tr∆∞·ªõc.")

    # Kh·ªüi t·∫°o predictor v√† n·∫°p models theo timestamp m·ªõi nh·∫•t (ho·∫∑c theo tham s·ªë)
    predictor = AdvancedTrafficPredictor()

    # T√¨m timestamp m·ªõi nh·∫•t n·∫øu ch∆∞a cung c·∫•p
    if not timestamp:
        ts = find_latest_timestamp(models_dir)
        if not ts:
            raise FileNotFoundError("No saved models found. Please run training to create models.")
        timestamp = ts

    ok = load_models_into_predictor(predictor, models_dir, timestamp)
    if not ok:
        raise RuntimeError("Failed to load models/scalers/metadata. Cannot perform prediction.")

    # G·ªçi v√†o h√†m suy lu·∫≠n ƒëi·ªÉm-th·ªùi-gian c·ªßa predictor
    return predictor.predict_from_inputs(
        when=when,
        current_tpm=current_tpm,
        previous_tpms=previous_tpms,
        response_time=response_time,
        push_notification_active=push_notification_active,
        minutes_since_push=minutes_since_push,
        minutes_ahead=minutes_ahead,
        extra_features=extra_features,
    )


def find_latest_timestamp(models_dir: str) -> str:
    """Find the latest metadata_<timestamp>.joblib and return its timestamp string."""
    pattern = os.path.join(models_dir, 'metadata_*.joblib')
    files = glob.glob(pattern)
    if not files:
        return None
    # Sort by timestamp parsed from filename or by mtime as fallback
    def extract_ts(path):
        base = os.path.basename(path)
        ts = base.replace('metadata_', '').replace('.joblib', '')
        return ts
    files_sorted = sorted(files, key=lambda p: extract_ts(p))
    return extract_ts(files_sorted[-1])


def load_models_into_predictor(predictor: AdvancedTrafficPredictor, models_dir: str, timestamp: str) -> bool:
    """Load best models, scalers, and metadata into the predictor for a given timestamp.
    Returns True if successful, False otherwise.
    """
    metadata_path = os.path.join(models_dir, f'metadata_{timestamp}.joblib')
    if not os.path.exists(metadata_path):
        print(f"‚ùå Metadata not found: {metadata_path}")
        return False

    metadata = joblib.load(metadata_path)
    best_models = metadata.get('best_models', {})
    feature_columns = metadata.get('feature_columns')
    target_columns = metadata.get('target_columns')

    if not best_models:
        print("‚ùå No best_models found in metadata; cannot load models.")
        return False

    # Initialize containers matching training structure
    predictor.models = {t: {} for t in target_columns}
    predictor.scalers = {}
    predictor.best_models = best_models
    predictor.model_performance = metadata.get('model_performance', {})

    # Load thresholds from metadata if available
    predictor.tpm_thresholds = metadata.get('tpm_thresholds') or metadata.get('thresholds')
    predictor.tpm_thresholds_stats = metadata.get('tpm_thresholds_stats')

    # If feature/target columns differ, update to match saved artifacts
    if feature_columns:
        predictor.feature_columns = feature_columns
    if target_columns:
        predictor.target_columns = target_columns

    # Th√¥ng tin t·ªïng quan tr∆∞·ªõc khi load t·ª´ng model
    print("\nüß† Model Summary")
    print(f"   ‚Ä¢ Timestamp: {timestamp}")
    print(f"   ‚Ä¢ Models dir: {models_dir}")
    print(f"   ‚Ä¢ #Features: {len(predictor.feature_columns) if predictor.feature_columns else 0}")
    if predictor.feature_columns:
        print(f"   ‚Ä¢ Feature columns: {predictor.feature_columns}")
    print(f"   ‚Ä¢ #Targets: {len(predictor.target_columns) if predictor.target_columns else 0}")
    if predictor.target_columns:
        print(f"   ‚Ä¢ Target columns: {predictor.target_columns}")

    # Load best model per target and its scaler
    for target, best_name in best_models.items():
        model_path = os.path.join(models_dir, f'{target}_{best_name}_{timestamp}.joblib')
        if not os.path.exists(model_path):
            print(f"‚ùå Missing model file: {model_path}")
            return False
        model_obj = joblib.load(model_path)
        predictor.models[target][best_name] = model_obj

        # Load scaler for the target (always saved, used particularly for SVR)
        scaler_path = os.path.join(models_dir, f'scaler_{target}_{timestamp}.joblib')
        scaler_loaded = False
        if not os.path.exists(scaler_path):
            print(f"‚ö†Ô∏è Scaler not found for {target}: {scaler_path}. Proceeding without scaler.")
        else:
            predictor.scalers[target] = joblib.load(scaler_path)
            scaler_loaded = True

        # In th√™m th√¥ng tin chi ti·∫øt cho t·ª´ng target/model
        model_class = type(model_obj).__name__
        print(f"\n   ‚û§ Target: {target}")
        print(f"     - Best model key: {best_name}")
        print(f"     - Estimator class: {model_class}")
        print(f"     - Model file: {os.path.basename(model_path)}")
        print(f"     - Scaler: {'loaded' if scaler_loaded else 'not found'}")

        # Th√¥ng tin hi·ªáu nƒÉng (n·∫øu c√≥ trong metadata.model_performance)
        perf_info = predictor.model_performance.get(target, {})
        best_perf = perf_info.get(best_name)
        if isinstance(best_perf, dict) and best_perf:
            metrics_str = ", ".join(f"{k}: {v:.4f}" if isinstance(v, (int, float)) else f"{k}: {v}"
                                    for k, v in best_perf.items())
            print(f"     - Performance: {metrics_str}")
        elif perf_info:
            print(f"     - Performance (raw): {perf_info}")

    # N·∫øu thresholds ch∆∞a c√≥, th·ª≠ t√≠nh t·ª´ training CSV
    if predictor.tpm_thresholds is None:
        try:
            if predictor.data_file_path and os.path.exists(predictor.data_file_path):
                df_train = predictor.load_real_data()
                predictor.tpm_thresholds = predictor.compute_tpm_thresholds_from_df(df_train)
                print(f"üßÆ Computed thresholds from training CSV: {predictor.tpm_thresholds}")
            else:
                print("‚ö†Ô∏è No thresholds in metadata and training CSV not found; labels may be 'unknown'.")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to compute thresholds from training CSV: {e}")

    # T√≥m t·∫Øt thresholds
    if predictor.tpm_thresholds:
        th = predictor.tpm_thresholds
        try:
            print(f"   ‚Ä¢ Thresholds: q60={th['q60']:.2f}, q80={th['q80']:.2f}, q90={th['q90']:.2f}")
        except Exception:
            print(f"   ‚Ä¢ Thresholds: {th}")

    print(f"\n‚úÖ Loaded models and metadata from timestamp {timestamp}")
    return True


def main():
    parser = argparse.ArgumentParser(description='Run demo predictions without retraining (using live New Relic data)')
    parser.add_argument('--hours-ahead', type=int, default=6, help='Hours ahead to scan for high TPM periods')
    parser.add_argument('--dataset', type=str, default=None, help='Path to dataset CSV as a fallback if API fails')
    parser.add_argument('--timestamp', type=str, default=None, help='Specific model timestamp to load (e.g., 20250809_224101)')
    parser.add_argument('--api-key', type=str, default=os.environ.get('NEWRELIC_API_KEY') or os.environ.get('NEW_RELIC_API_KEY'), help='New Relic API key (default from env NEWRELIC_API_KEY)')
    parser.add_argument('--app-id', type=str, default=os.environ.get('NEWRELIC_APP_ID') or os.environ.get('NEW_RELIC_APP_ID'), help='New Relic Application ID (default from env NEWRELIC_APP_ID)')
    parser.add_argument('--metric', type=str, default='HttpDispatcher', help='Metric name to query (default: HttpDispatcher)')
    args = parser.parse_args()

    models_dir = os.path.join(CURRENT_DIR, 'models')
    if not os.path.isdir(models_dir):
        print(f"‚ùå Models directory not found: {models_dir}\nPlease run the training pipeline first to generate models.")
        sys.exit(1)

    # Determine timestamp to load
    timestamp = args.timestamp or find_latest_timestamp(models_dir)
    if not timestamp:
        print("‚ùå No saved models found. Please run training to create models.")
        sys.exit(1)

    # Initialize predictor
    predictor = AdvancedTrafficPredictor()

    # Prefer live New Relic data (last 3 hours, 1-min granularity)
    print("üåê Fetching recent 3h data (1-min) from New Relic API...")
    live_df = get_recent_3h_1min_dataframe(api_key=args.api_key, app_id=args.app_id, metric_name=args.metric)

    if live_df is None or live_df.empty:
        if args.dataset:
            print("‚ö†Ô∏è API data unavailable. Falling back to dataset CSV provided via --dataset.")
            predictor.data_file_path = args.dataset
            df = predictor.load_and_preprocess_data()
        else:
            print("‚ùå No live data fetched and no dataset fallback provided. Aborting.")
            sys.exit(1)
    else:
        # Gi·∫£ l·∫≠p push v·ª´a di·ªÖn ra t·∫°i th·ªùi ƒëi·ªÉm cu·ªëi c√πng trong live_df
        # push_time = live_df['timestamp'].max()
        # window_mins = 15  # ƒë·ªô d√†i kho·∫£ng th·ªùi gian "active"
        #
        # # T√≠nh ph√∫t k·ªÉ t·ª´ push (√¢m tr∆∞·ªõc push, >=0 sau push)
        # delta_min = (live_df['timestamp'] - push_time).dt.total_seconds() / 60.0
        #
        # live_df['minutes_since_push'] = delta_min.where(delta_min >= 0, other=np.nan)
        # # Active trong 15 ph√∫t sau push
        # live_df['push_notification_active'] = ((delta_min >= 0) & (delta_min <= window_mins)).astype(int)
        #
        # # ƒêi·ªÅn gi√° tr·ªã m·∫∑c ƒë·ªãnh cho c√°c d√≤ng tr∆∞·ªõc push
        # live_df['minutes_since_push'] = live_df['minutes_since_push'].fillna(999999)

        # Create features from live data for prediction (keep latest rows even if some targets are NaN)
        df = predictor.create_features(live_df)

    # Load models into predictor
    ok = load_models_into_predictor(predictor, models_dir, timestamp)
    if not ok:
        print("‚ùå Failed to load models. Aborting.")
        sys.exit(1)

    # Run the demo predictions without retraining
    print("\nüöÄ Running independent demo predictions...")
    predictor.demonstrate_predictions(df, hours_ahead=args.hours_ahead)


def main2():
    # L·∫•y ng√†y h√¥m nay 09:00 t·∫°i GMT+7
    tz = pytz.timezone('Asia/Bangkok')
    date_str = '2025-08-10 15:30'
    naive_dt = datetime.strptime(date_str, '%Y-%m-%d %H:%M')
    test_when = tz.localize(naive_dt)

    # Gi√° tr·ªã m·∫´u ƒë·ªÉ test
    current_tpm = 125.0
    previous_tpms = [120.0, 100.0, 123.0, 124.0, 135.0,120.0, 100.0, 123.0, 124.0, 135.0,120.0, 100.0, 123.0, 124.0, 135.0]  # 1‚Üí5 ph√∫t tr∆∞·ªõc
    response_time = 120.0

    # M√¥ ph·ªèng v·ª´a push l√∫c 09:00
    push_notification_active = 0
    minutes_since_push = 3000

    print("\nüß™ Testing predict_from_inputs at 09:00 GMT+7 ...")
    result = predict_from_inputs(
        when=test_when,
        current_tpm=current_tpm,
        previous_tpms=previous_tpms,
        response_time=response_time,
        push_notification_active=push_notification_active,
        minutes_since_push=minutes_since_push,
        minutes_ahead=(5, 10, 15),
        # models_dir=None, timestamp=None  # d√πng m·∫∑c ƒë·ªãnh: l·∫•y model m·ªõi nh·∫•t t·ª´ th∆∞ m·ª•c models
    )
    preds = result.get('predictions', {})
    labels = result.get('labels', {})

    print(f"üß™ Predictions at {test_when.strftime('%Y-%m-%d %H:%M %Z')}:")
    for key in sorted(preds.keys(), key=lambda k: int(k.split('_')[1].replace('min', ''))):
        minutes = key.split('_')[1].replace('min', '')
        value = preds[key]
        label = labels.get(key, 'unknown')
        print(f"  ‚Ä¢ t+{minutes}m: {value:.2f} ({label})")


if __name__ == '__main__':
    # main2()
    main()
